{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Algorithm Analysis\n",
    "\n",
    "This notebook demonstrates how to run and compare different clustering algorithms using our framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from clustering_analysis import (\n",
    "    ClusteringExperiment,\n",
    "    KMeansClusterer, FuzzyCMeansClusterer, GaussianMixtureClusterer,\n",
    "    DBSCANClusterer, SpectralClusterer,\n",
    "    SyntheticDataGenerator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sample Dataset\n",
    "\n",
    "Let's load a sample dataset for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data generator\n",
    "generator = SyntheticDataGenerator(\"../data/synthetic\")\n",
    "\n",
    "# Generate a sample dataset\n",
    "bar_omega = 0.1\n",
    "K = 3\n",
    "p = 2  # 2D for visualization\n",
    "n = 1000\n",
    "\n",
    "X, y_true = generator.generate_dataset(bar_omega, K, p, n, random_state=42)\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"True clusters: {len(np.unique(y_true))}\")\n",
    "print(f\"Cluster distribution: {dict(zip(*np.unique(y_true, return_counts=True)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "for cluster_id in np.unique(y_true):\n",
    "    mask = y_true == cluster_id\n",
    "    plt.scatter(X[mask, 0], X[mask, 1], \n",
    "               c=colors[cluster_id], alpha=0.7, \n",
    "               label=f'True Cluster {cluster_id}', s=30)\n",
    "\n",
    "plt.title(f'Original Dataset (BarOmega = {bar_omega})')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Algorithm Testing\n",
    "\n",
    "Let's test each clustering algorithm individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test K-Means\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Initialize and fit K-Means\n",
    "kmeans = KMeansClusterer(n_clusters=K)\n",
    "kmeans.fit(X_scaled)\n",
    "y_kmeans = kmeans.labels_\n",
    "\n",
    "print(f\"K-Means execution time: {kmeans.get_execution_time():.4f} seconds\")\n",
    "print(f\"Predicted clusters: {len(np.unique(y_kmeans))}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# True labels\n",
    "plt.subplot(1, 2, 1)\n",
    "for cluster_id in np.unique(y_true):\n",
    "    mask = y_true == cluster_id\n",
    "    plt.scatter(X[mask, 0], X[mask, 1], c=colors[cluster_id], alpha=0.7, s=30)\n",
    "plt.title('True Labels')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# K-Means results\n",
    "plt.subplot(1, 2, 2)\n",
    "for cluster_id in np.unique(y_kmeans):\n",
    "    mask = y_kmeans == cluster_id\n",
    "    plt.scatter(X[mask, 0], X[mask, 1], c=colors[cluster_id], alpha=0.7, s=30)\n",
    "plt.title('K-Means Results')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test DBSCAN with parameter optimization\n",
    "dbscan = DBSCANClusterer(n_clusters=K)\n",
    "dbscan.fit(X_scaled, optimize=True, true_labels=y_true)\n",
    "y_dbscan = dbscan.labels_\n",
    "\n",
    "print(f\"DBSCAN execution time: {dbscan.get_execution_time():.4f} seconds\")\n",
    "print(f\"Best parameters: {dbscan.best_params}\")\n",
    "print(f\"Predicted clusters: {len(np.unique(y_dbscan[y_dbscan != -1]))} (excluding noise)\")\n",
    "print(f\"Noise points: {sum(y_dbscan == -1)}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# True labels\n",
    "plt.subplot(1, 2, 1)\n",
    "for cluster_id in np.unique(y_true):\n",
    "    mask = y_true == cluster_id\n",
    "    plt.scatter(X[mask, 0], X[mask, 1], c=colors[cluster_id], alpha=0.7, s=30)\n",
    "plt.title('True Labels')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# DBSCAN results\n",
    "plt.subplot(1, 2, 2)\n",
    "# Plot noise points in black\n",
    "noise_mask = y_dbscan == -1\n",
    "if np.any(noise_mask):\n",
    "    plt.scatter(X[noise_mask, 0], X[noise_mask, 1], c='black', alpha=0.5, s=10, label='Noise')\n",
    "\n",
    "# Plot clusters\n",
    "for cluster_id in np.unique(y_dbscan):\n",
    "    if cluster_id != -1:\n",
    "        mask = y_dbscan == cluster_id\n",
    "        plt.scatter(X[mask, 0], X[mask, 1], c=colors[cluster_id % len(colors)], alpha=0.7, s=30)\n",
    "\n",
    "plt.title('DBSCAN Results')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "if np.any(noise_mask):\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Algorithm Comparison\n",
    "\n",
    "Let's run all algorithms and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all algorithms\n",
    "algorithms = {\n",
    "    'K-Means': KMeansClusterer(n_clusters=K),\n",
    "    'Fuzzy C-Means': FuzzyCMeansClusterer(n_clusters=K),\n",
    "    'Gaussian Mixture': GaussianMixtureClusterer(n_clusters=K),\n",
    "    'DBSCAN': DBSCANClusterer(n_clusters=K),\n",
    "    'Spectral': SpectralClusterer(n_clusters=K)\n",
    "}\n",
    "\n",
    "# Run all algorithms\n",
    "results = {}\n",
    "predictions = {}\n",
    "\n",
    "for name, algorithm in algorithms.items():\n",
    "    print(f\"\\nRunning {name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Fit algorithm\n",
    "        if name in ['DBSCAN', 'Spectral']:\n",
    "            algorithm.fit(X_scaled, optimize=True, true_labels=y_true)\n",
    "        else:\n",
    "            algorithm.fit(X_scaled)\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'execution_time': algorithm.get_execution_time(),\n",
    "            'n_clusters': len(np.unique(algorithm.labels_[algorithm.labels_ != -1])),\n",
    "            'algorithm': algorithm\n",
    "        }\n",
    "        predictions[name] = algorithm.labels_\n",
    "        \n",
    "        print(f\"  Execution time: {results[name]['execution_time']:.4f} seconds\")\n",
    "        print(f\"  Clusters found: {results[name]['n_clusters']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        results[name] = {'error': str(e)}\n",
    "        predictions[name] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all results\n",
    "n_algorithms = len([name for name, pred in predictions.items() if pred is not None])\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Plot true labels first\n",
    "ax = axes[0]\n",
    "for cluster_id in np.unique(y_true):\n",
    "    mask = y_true == cluster_id\n",
    "    ax.scatter(X[mask, 0], X[mask, 1], c=colors[cluster_id], alpha=0.7, s=20)\n",
    "ax.set_title('True Labels')\n",
    "ax.set_xlabel('Feature 1')\n",
    "ax.set_ylabel('Feature 2')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot algorithm results\n",
    "plot_idx = 1\n",
    "for name, y_pred in predictions.items():\n",
    "    if y_pred is not None and plot_idx < len(axes):\n",
    "        ax = axes[plot_idx]\n",
    "        \n",
    "        # Handle noise points (DBSCAN)\n",
    "        if -1 in y_pred:\n",
    "            noise_mask = y_pred == -1\n",
    "            ax.scatter(X[noise_mask, 0], X[noise_mask, 1], \n",
    "                      c='black', alpha=0.3, s=10)\n",
    "        \n",
    "        # Plot clusters\n",
    "        unique_labels = np.unique(y_pred)\n",
    "        for i, cluster_id in enumerate(unique_labels):\n",
    "            if cluster_id != -1:\n",
    "                mask = y_pred == cluster_id\n",
    "                ax.scatter(X[mask, 0], X[mask, 1], \n",
    "                          c=colors[i % len(colors)], alpha=0.7, s=20)\n",
    "        \n",
    "        # Add execution time to title\n",
    "        exec_time = results[name].get('execution_time', 0)\n",
    "        ax.set_title(f'{name}\\n({exec_time:.3f}s)')\n",
    "        ax.set_xlabel('Feature 1')\n",
    "        ax.set_ylabel('Feature 2')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plot_idx += 1\n",
    "\n",
    "# Hide unused subplots\n",
    "for i in range(plot_idx, len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate and Compare Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for each algorithm\n",
    "from clustering_analysis import ClusteringMetrics\n",
    "\n",
    "metrics_calculator = ClusteringMetrics()\n",
    "metrics_results = []\n",
    "\n",
    "for name, y_pred in predictions.items():\n",
    "    if y_pred is not None:\n",
    "        print(f\"\\nCalculating metrics for {name}...\")\n",
    "        \n",
    "        # Calculate all metrics\n",
    "        metrics = metrics_calculator.calculate_all_metrics(\n",
    "            X_scaled, y_pred, y_true\n",
    "        )\n",
    "        \n",
    "        # Add algorithm name and execution time\n",
    "        metrics['algorithm'] = name\n",
    "        metrics['execution_time'] = results[name].get('execution_time', 0)\n",
    "        metrics_results.append(metrics)\n",
    "        \n",
    "        # Print key metrics\n",
    "        print(f\"  Adjusted Rand Score: {metrics.get('adjusted_rand_score', 'N/A'):.3f}\")\n",
    "        print(f\"  Silhouette Score: {metrics.get('silhouette_score', 'N/A'):.3f}\")\n",
    "        print(f\"  Execution Time: {metrics['execution_time']:.3f}s\")\n",
    "\n",
    "# Create DataFrame for easy comparison\n",
    "metrics_df = pd.DataFrame(metrics_results)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"METRICS COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "display(metrics_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance comparison plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Adjusted Rand Score\n",
    "if 'adjusted_rand_score' in metrics_df.columns:\n",
    "    axes[0, 0].bar(metrics_df['algorithm'], metrics_df['adjusted_rand_score'])\n",
    "    axes[0, 0].set_title('Adjusted Rand Score')\n",
    "    axes[0, 0].set_ylabel('Score')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Silhouette Score\n",
    "if 'silhouette_score' in metrics_df.columns:\n",
    "    axes[0, 1].bar(metrics_df['algorithm'], metrics_df['silhouette_score'])\n",
    "    axes[0, 1].set_title('Silhouette Score')\n",
    "    axes[0, 1].set_ylabel('Score')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Execution Time\n",
    "axes[1, 0].bar(metrics_df['algorithm'], metrics_df['execution_time'])\n",
    "axes[1, 0].set_title('Execution Time')\n",
    "axes[1, 0].set_ylabel('Time (seconds)')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Performance vs Time scatter\n",
    "if 'adjusted_rand_score' in metrics_df.columns:\n",
    "    axes[1, 1].scatter(metrics_df['execution_time'], metrics_df['adjusted_rand_score'])\n",
    "    for i, txt in enumerate(metrics_df['algorithm']):\n",
    "        axes[1, 1].annotate(txt, (metrics_df['execution_time'].iloc[i], \n",
    "                                 metrics_df['adjusted_rand_score'].iloc[i]))\n",
    "    axes[1, 1].set_xlabel('Execution Time (seconds)')\n",
    "    axes[1, 1].set_ylabel('Adjusted Rand Score')\n",
    "    axes[1, 1].set_title('Performance vs Speed')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Recommendations\n",
    "\n",
    "Based on the analysis above, let's summarize the performance of each algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_data = []\n",
    "\n",
    "for _, row in metrics_df.iterrows():\n",
    "    algorithm = row['algorithm']\n",
    "    \n",
    "    # Performance rating based on ARI\n",
    "    ari = row.get('adjusted_rand_score', 0)\n",
    "    if ari > 0.8:\n",
    "        performance = \"Excellent\"\n",
    "    elif ari > 0.6:\n",
    "        performance = \"Good\"\n",
    "    elif ari > 0.4:\n",
    "        performance = \"Fair\"\n",
    "    else:\n",
    "        performance = \"Poor\"\n",
    "    \n",
    "    # Speed rating\n",
    "    exec_time = row['execution_time']\n",
    "    if exec_time < 0.01:\n",
    "        speed = \"Very Fast\"\n",
    "    elif exec_time < 0.1:\n",
    "        speed = \"Fast\"\n",
    "    elif exec_time < 1.0:\n",
    "        speed = \"Moderate\"\n",
    "    else:\n",
    "        speed = \"Slow\"\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Algorithm': algorithm,\n",
    "        'Performance': performance,\n",
    "        'Speed': speed,\n",
    "        'ARI': f\"{ari:.3f}\",\n",
    "        'Silhouette': f\"{row.get('silhouette_score', 0):.3f}\",\n",
    "        'Time (s)': f\"{exec_time:.4f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"ALGORITHM PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. **Loaded** a synthetic dataset with controlled cluster overlap\n",
    "2. **Applied** five different clustering algorithms\n",
    "3. **Visualized** the clustering results\n",
    "4. **Calculated** comprehensive evaluation metrics\n",
    "5. **Compared** algorithm performance\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "The performance of clustering algorithms depends heavily on:\n",
    "- **Dataset characteristics** (overlap, shape, density)\n",
    "- **Algorithm assumptions** (spherical vs arbitrary shapes)\n",
    "- **Parameter tuning** (especially for DBSCAN and Spectral)\n",
    "- **Data preprocessing** (standardization importance)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Test on datasets with different overlap levels\n",
    "2. Analyze performance vs dataset size and dimensions\n",
    "3. Explore real-world datasets\n",
    "4. Fine-tune algorithm parameters\n",
    "\n",
    "This framework provides a systematic approach to clustering algorithm evaluation and comparison."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}