{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Results Visualization\n",
    "\n",
    "This notebook demonstrates how to load and visualize clustering experiment results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from clustering_analysis.visualization import ClusteringVisualizer\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Experiment Results\n",
    "\n",
    "First, let's load the results from our clustering experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "results_dir = Path('../data/results/metrics')\n",
    "figures_dir = Path('../data/results/figures')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load all results files\n",
    "results_files = list(results_dir.glob(\"*_metrics.csv\"))\n",
    "print(f\"Found {len(results_files)} results files:\")\n",
    "for f in results_files:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "# If no results files, create sample data\n",
    "if not results_files:\n",
    "    print(\"\\nNo results files found. Creating sample data...\")\n",
    "    \n",
    "    # Create sample results\n",
    "    algorithms = ['K-Means', 'Fuzzy C-Means', 'Gaussian Mixture', 'DBSCAN', 'Spectral']\n",
    "    bar_omega_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    \n",
    "    sample_results = []\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    for algorithm in algorithms:\n",
    "        for bar_omega in bar_omega_values:\n",
    "            # Simulate performance degradation with overlap\n",
    "            base_ari = 0.95 - (bar_omega * 1.2) + np.random.normal(0, 0.05)\n",
    "            base_sil = 0.7 - (bar_omega * 0.8) + np.random.normal(0, 0.05)\n",
    "            \n",
    "            # Algorithm-specific adjustments\n",
    "            if algorithm == 'DBSCAN':\n",
    "                base_ari += 0.1 if bar_omega > 0.2 else -0.1\n",
    "            elif algorithm == 'K-Means':\n",
    "                base_ari -= 0.1 if bar_omega > 0.3 else 0\n",
    "            \n",
    "            sample_results.append({\n",
    "                'algorithm': algorithm,\n",
    "                'experiment': 'bar_omega_variation',\n",
    "                'bar_omega': bar_omega,\n",
    "                'K': 3,\n",
    "                'P': 5,\n",
    "                'N': 5000,\n",
    "                'adjusted_rand_score': max(0, min(1, base_ari)),\n",
    "                'silhouette_score': max(-1, min(1, base_sil)),\n",
    "                'fit_time': np.random.exponential(0.1),\n",
    "                'dunn_index': max(0, np.random.gamma(2, 0.1)),\n",
    "                'calinski_harabasz_score': np.random.gamma(100, 1)\n",
    "            })\n",
    "    \n",
    "    # Save sample results\n",
    "    sample_df = pd.DataFrame(sample_results)\n",
    "    sample_file = results_dir / 'sample_metrics.csv'\n",
    "    sample_df.to_csv(sample_file, index=False)\n",
    "    \n",
    "    print(f\"Created sample data with {len(sample_results)} results\")\n",
    "    results_files = [sample_file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and combine all results\n",
    "all_results = []\n",
    "\n",
    "for results_file in results_files:\n",
    "    print(f\"Loading {results_file.name}...\")\n",
    "    df = pd.read_csv(results_file)\n",
    "    all_results.extend(df.to_dict('records'))\n",
    "    print(f\"  Loaded {len(df)} results\")\n",
    "\n",
    "print(f\"\\nTotal results loaded: {len(all_results)}\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "print(f\"Columns: {list(results_df.columns)}\")\n",
    "print(f\"Algorithms: {results_df['algorithm'].unique()}\")\n",
    "\n",
    "# Display first few rows\n",
    "display(results_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Performance Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create overview statistics\n",
    "if 'adjusted_rand_score' in results_df.columns:\n",
    "    performance_summary = results_df.groupby('algorithm')['adjusted_rand_score'].agg([\n",
    "        'count', 'mean', 'std', 'min', 'max'\n",
    "    ]).round(3)\n",
    "    \n",
    "    print(\"Algorithm Performance Summary (Adjusted Rand Index):\")\n",
    "    display(performance_summary)\n",
    "\n",
    "# Execution time summary\n",
    "if 'fit_time' in results_df.columns:\n",
    "    time_summary = results_df.groupby('algorithm')['fit_time'].agg([\n",
    "        'count', 'mean', 'std', 'min', 'max'\n",
    "    ]).round(4)\n",
    "    \n",
    "    print(\"\\nExecution Time Summary (seconds):\")\n",
    "    display(time_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualizer\n",
    "visualizer = ClusteringVisualizer(output_dir=str(figures_dir))\n",
    "\n",
    "# 1. Algorithm comparison boxplot\n",
    "if 'adjusted_rand_score' in results_df.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    sns.boxplot(data=results_df, x='algorithm', y='adjusted_rand_score')\n",
    "    plt.title('Algorithm Performance Comparison\\n(Adjusted Rand Index)', fontsize=14)\n",
    "    plt.ylabel('Adjusted Rand Index', fontsize=12)\n",
    "    plt.xlabel('Algorithm', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add mean markers\n",
    "    means = results_df.groupby('algorithm')['adjusted_rand_score'].mean()\n",
    "    for i, (algorithm, mean_val) in enumerate(means.items()):\n",
    "        plt.scatter(i, mean_val, color='red', s=100, zorder=5, marker='D')\n",
    "        plt.text(i, mean_val + 0.02, f'{mean_val:.3f}', ha='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Omega sensitivity analysis\n",
    "if 'bar_omega' in results_df.columns and results_df['bar_omega'].nunique() > 1:\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Filter for specific metrics\n",
    "    metric_cols = ['adjusted_rand_score', 'silhouette_score']\n",
    "    available_metrics = [col for col in metric_cols if col in results_df.columns]\n",
    "    \n",
    "    for i, metric in enumerate(available_metrics, 1):\n",
    "        plt.subplot(2, 2, i)\n",
    "        \n",
    "        # Line plot for each algorithm\n",
    "        for algorithm in results_df['algorithm'].unique():\n",
    "            algo_data = results_df[results_df['algorithm'] == algorithm]\n",
    "            \n",
    "            if len(algo_data) > 1:\n",
    "                # Group by bar_omega and calculate mean and std\n",
    "                grouped = algo_data.groupby('bar_omega')[metric].agg(['mean', 'std']).reset_index()\n",
    "                \n",
    "                plt.errorbar(grouped['bar_omega'], grouped['mean'], \n",
    "                           yerr=grouped['std'], label=algorithm, marker='o', capsize=3)\n",
    "        \n",
    "        plt.xlabel('Bar Omega (Cluster Overlap)', fontsize=10)\n",
    "        plt.ylabel(metric.replace('_', ' ').title(), fontsize=10)\n",
    "        plt.title(f'{metric.replace(\"_\", \" \").title()} vs Cluster Overlap', fontsize=11)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Execution time analysis\n",
    "    if 'fit_time' in results_df.columns:\n",
    "        plt.subplot(2, 2, len(available_metrics) + 1)\n",
    "        \n",
    "        for algorithm in results_df['algorithm'].unique():\n",
    "            algo_data = results_df[results_df['algorithm'] == algorithm]\n",
    "            grouped = algo_data.groupby('bar_omega')['fit_time'].mean().reset_index()\n",
    "            plt.plot(grouped['bar_omega'], grouped['fit_time'], \n",
    "                    label=algorithm, marker='o')\n",
    "        \n",
    "        plt.xlabel('Bar Omega (Cluster Overlap)', fontsize=10)\n",
    "        plt.ylabel('Execution Time (seconds)', fontsize=10)\n",
    "        plt.title('Execution Time vs Cluster Overlap', fontsize=11)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance vs Speed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance vs execution time scatter plot\n",
    "if 'adjusted_rand_score' in results_df.columns and 'fit_time' in results_df.columns:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Calculate mean performance and time for each algorithm\n",
    "    algo_summary = results_df.groupby('algorithm').agg({\n",
    "        'adjusted_rand_score': 'mean',\n",
    "        'fit_time': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Scatter plot\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(algo_summary)))\n",
    "    \n",
    "    for i, (_, row) in enumerate(algo_summary.iterrows()):\n",
    "        plt.scatter(row['fit_time'], row['adjusted_rand_score'], \n",
    "                   s=200, alpha=0.7, color=colors[i], label=row['algorithm'])\n",
    "        \n",
    "        # Add algorithm name near point\n",
    "        plt.annotate(row['algorithm'], \n",
    "                    (row['fit_time'], row['adjusted_rand_score']),\n",
    "                    xytext=(10, 5), textcoords='offset points', fontsize=10)\n",
    "    \n",
    "    plt.xlabel('Average Execution Time (seconds)', fontsize=12)\n",
    "    plt.ylabel('Average Adjusted Rand Index', fontsize=12)\n",
    "    plt.title('Algorithm Performance vs Speed Trade-off', fontsize=14)\n",
    "    \n",
    "    # Add quadrant lines\n",
    "    mean_time = algo_summary['fit_time'].mean()\n",
    "    mean_performance = algo_summary['adjusted_rand_score'].mean()\n",
    "    \n",
    "    plt.axhline(y=mean_performance, color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.axvline(x=mean_time, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Add quadrant labels\n",
    "    plt.text(mean_time * 0.1, mean_performance * 1.05, 'Fast & Good', \n",
    "             fontsize=12, fontweight='bold', ha='left')\n",
    "    plt.text(mean_time * 1.5, mean_performance * 1.05, 'Slow & Good', \n",
    "             fontsize=12, fontweight='bold', ha='left')\n",
    "    plt.text(mean_time * 0.1, mean_performance * 0.95, 'Fast & Poor', \n",
    "             fontsize=12, fontweight='bold', ha='left')\n",
    "    plt.text(mean_time * 1.5, mean_performance * 0.95, 'Slow & Poor', \n",
    "             fontsize=12, fontweight='bold', ha='left')\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Comprehensive Visualization Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the visualizer to create comprehensive report\n",
    "print(\"Creating comprehensive visualization report...\")\n",
    "\n",
    "try:\n",
    "    visualizer.create_comprehensive_report(all_results)\n",
    "    print(f\"\\nComprehensive report created in: {figures_dir}\")\n",
    "    \n",
    "    # List generated files\n",
    "    generated_files = list(figures_dir.glob(\"*.png\"))\n",
    "    print(f\"\\nGenerated {len(generated_files)} visualization files:\")\n",
    "    for f in generated_files:\n",
    "        print(f\"  - {f.name}\")\n",
    "        \n",
    "    # Check if summary report exists\n",
    "    report_file = figures_dir / 'analysis_report.md'\n",
    "    if report_file.exists():\n",
    "        print(f\"\\nAnalysis report: {report_file}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error creating comprehensive report: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Ranking Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create algorithm ranking based on different criteria\n",
    "ranking_metrics = ['adjusted_rand_score', 'silhouette_score', 'fit_time']\n",
    "available_ranking_metrics = [m for m in ranking_metrics if m in results_df.columns]\n",
    "\n",
    "if available_ranking_metrics:\n",
    "    print(\"Algorithm Ranking Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    rankings = {}\n",
    "    \n",
    "    for metric in available_ranking_metrics:\n",
    "        # Calculate mean scores\n",
    "        metric_means = results_df.groupby('algorithm')[metric].mean().sort_values(\n",
    "            ascending=(metric == 'fit_time')  # Lower is better for time\n",
    "        )\n",
    "        \n",
    "        rankings[metric] = metric_means\n",
    "        \n",
    "        print(f\"\\n{metric.replace('_', ' ').title()} Ranking:\")\n",
    "        for rank, (algorithm, score) in enumerate(metric_means.items(), 1):\n",
    "            print(f\"  {rank}. {algorithm}: {score:.4f}\")\n",
    "    \n",
    "    # Create ranking visualization\n",
    "    if len(available_ranking_metrics) > 1:\n",
    "        fig, axes = plt.subplots(1, len(available_ranking_metrics), \n",
    "                                figsize=(5 * len(available_ranking_metrics), 6))\n",
    "        \n",
    "        if len(available_ranking_metrics) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, metric in enumerate(available_ranking_metrics):\n",
    "            ranking = rankings[metric]\n",
    "            \n",
    "            bars = axes[i].barh(range(len(ranking)), ranking.values)\n",
    "            axes[i].set_yticks(range(len(ranking)))\n",
    "            axes[i].set_yticklabels(ranking.index)\n",
    "            axes[i].set_xlabel(metric.replace('_', ' ').title())\n",
    "            axes[i].set_title(f'Ranking by {metric.replace(\"_\", \" \").title()}')\n",
    "            \n",
    "            # Color bars by rank\n",
    "            colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(ranking)))\n",
    "            for bar, color in zip(bars, colors):\n",
    "                bar.set_color(color)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics and conclusions\n",
    "print(\"Clustering Analysis Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total experiments analyzed: {len(results_df)}\")\n",
    "print(f\"Algorithms compared: {', '.join(results_df['algorithm'].unique())}\")\n",
    "print(f\"Unique experimental conditions: {len(results_df.drop_duplicates(['bar_omega', 'K', 'P', 'N']))}\")\n",
    "\n",
    "# Best performing algorithm overall\n",
    "if 'adjusted_rand_score' in results_df.columns:\n",
    "    best_algorithm = results_df.groupby('algorithm')['adjusted_rand_score'].mean().idxmax()\n",
    "    best_score = results_df.groupby('algorithm')['adjusted_rand_score'].mean().max()\n",
    "    print(f\"\\nBest performing algorithm: {best_algorithm} (ARI: {best_score:.3f})\")\n",
    "\n",
    "# Fastest algorithm\n",
    "if 'fit_time' in results_df.columns:\n",
    "    fastest_algorithm = results_df.groupby('algorithm')['fit_time'].mean().idxmin()\n",
    "    fastest_time = results_df.groupby('algorithm')['fit_time'].mean().min()\n",
    "    print(f\"Fastest algorithm: {fastest_algorithm} ({fastest_time:.4f} seconds)\")\n",
    "\n",
    "# Most consistent algorithm (lowest std in performance)\n",
    "if 'adjusted_rand_score' in results_df.columns:\n",
    "    most_consistent = results_df.groupby('algorithm')['adjusted_rand_score'].std().idxmin()\n",
    "    consistency_score = results_df.groupby('algorithm')['adjusted_rand_score'].std().min()\n",
    "    print(f\"Most consistent algorithm: {most_consistent} (std: {consistency_score:.3f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Analysis Complete!\")\n",
    "print(f\"Visualizations saved in: {figures_dir}\")\n",
    "print(\"\\nRecommendations:\")\n",
    "print(\"1. Check algorithm-specific performance under different overlap conditions\")\n",
    "print(\"2. Consider computational constraints when selecting algorithms\")\n",
    "print(\"3. Validate results on real datasets before final selection\")\n",
    "print(\"4. Fine-tune parameters for best-performing algorithms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}